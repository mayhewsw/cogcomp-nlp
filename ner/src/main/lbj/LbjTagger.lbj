package edu.illinois.cs.cogcomp.ner.LbjFeatures;

import java.util.*;

import edu.illinois.cs.cogcomp.ner.LbjTagger.NEWord;
import edu.illinois.cs.cogcomp.ner.LanguageSpecificNormalizer;
import edu.illinois.cs.cogcomp.ner.ExpressiveFeatures.WordTopicAndLayoutFeatures;
import edu.illinois.cs.cogcomp.ner.ExpressiveFeatures.BrownClusters;
import edu.illinois.cs.cogcomp.ner.ExpressiveFeatures.Gazetteers;
import edu.illinois.cs.cogcomp.ner.ExpressiveFeatures.WordEmbeddings;
import edu.illinois.cs.cogcomp.ner.LbjTagger.ParametersForLbjCode;
import edu.illinois.cs.cogcomp.ner.StringStatisticsUtils.*;
import edu.illinois.cs.cogcomp.ner.WordEmbedding;

  
//---------------- CLASSIFIER LEVEL 1 -------------------

discrete% posTag(NEWord word) <-
{
	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("POSTags")){
 		int i;
        NEWord w = word, last = word;
        for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
        for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

        for (; w != last; w = (NEWord) w.next) sense "POS" + i++ : w.partOfSpeech;
	}
}


real% wikifierFeats(NEWord word) <-
    {
    if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("Wikifier")){
           if(word.wikifierfeats != null){
               for(int j = 0; j < word.wikifierfeats.length; j++){
                   String f = word.wikifierfeats[j];
                   String[] sf = f.split(":");
                   String name = "";
                   for(int i = 0; i < sf.length-1; i++)
                       name += sf[i]+":";
                   sense "wiki:" + name : Double.parseDouble(sf[sf.length-1]);
               }
           }
       }
    }


real% wikifierConjunction(NEWord word) <-
{
	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("Wikifier")){
        if(word.wikifierfeats != null){
            for(int i = 0; i < word.wikifierfeats.length; i++){
                String[] sf = word.wikifierfeats[i].split(":");
                String name = sf[0];
                Double value = Double.parseDouble(sf[1]);
                for(int j = 0; j < word.wikifierfeats.length; j++){
                    String[] sf1 = word.wikifierfeats[j].split(":");
                    String name1 = sf1[0];
                    Double value1 = Double.parseDouble(sf1[1]);
                    sense "wiki" + name +"_x_"+name1: value*value1;
                }
            }
        }
	}
}

real% PreTagWikifierConjunction(NEWord word) <-
{
	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("Wikifier"))
	{
	  	NEWord w = word;
	  	int i;
	  	if(w.previous!=null && w.wikifierfeats!=null)
	  	{
	  	    String label = null;
			if (NETaggerLevel1.isTraining/*&&ParametersForLbjCode.currentParameters.trainingIteration==0*/)
	    		label = ((NEWord)w.previous).neLabel;
	    	else
	    		label = ((NEWord)w.previous).neTypeLevel1;

            for(i = 0; i < w.wikifierfeats.length; i++){
                String f = w.wikifierfeats[i];
                String[] sf = f.split(":");
                sense "wikifier" + sf[0] + "-" + label : Double.parseDouble(sf[1]);
            }
      	}
    }
}

real% CurrentEmbedding(NEWord word) <-
{
	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("Embedding")){
        if(word.wordvec != null){
            for(int i = 0; i < word.wordvec.length; i++){
                Double f = word.wordvec[i];
                sense "CCAWordEmbedding" + i : f;
            }
        }else{
           for(int i = 0; i < 100; i++){
                  sense "CCAWordEmbedding" + i : 0.0;
           }

        }
	}
}

real% PrevEmbedding(NEWord word) <-
{
	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("Embedding")){
	    if(word.previous!=null){
	        NEWord w = (NEWord)word.previous;
	        if(w.wordvec !=null){
                for(int i = 0; i < w.wordvec.length; i++){
                    Double f = w.wordvec[i];
                    sense "PreCCAWordEmbedding" + i : f;
                }
            }
        }
	}
}


/*
real% PrevEmbeddingAvg(NEWord word) <-
{
	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("Embedding")){

	    List vecs_ = new ArrayList();
	    List<Double[]> vecs = (List<Double[]>) vecs_;

  	    for (i = 0; i > -3 && w.previous != null; --i){
  	        NEWord w = (NEWord) w.previous;
  	        if(w.wordvec!=null) vecs.add(w.wordvec);
        }
        Double[] avg = WordEmbedding.averageVectors(vecs);
        for(int i = 0; i < avg.length; i++){
            sense "PreCCAWordEmbeddingAvg" + i : avg[i];
        }
    }
}
*/

real% NextEmbedding(NEWord word) <-
{
	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("Embedding")){
	    if(word.next!=null){
	        NEWord w = (NEWord)word.next;
	        if(w.wordvec !=null){
                for(int i = 0; i < w.wordvec.length; i++){
                    Double f = w.wordvec[i];
                    sense "NextCCAWordEmbedding" + i : f;
                }
            }
        }
	}
}

real% PreTagEmbeddingConjunction(NEWord word) <-
{
	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("PreviousTag1"))
	{
	  	NEWord w = word;
	  	int i;
	  	if(w.previous!=null && w.wordvec!=null)
	  	{
	  	    String label = null;
			if (NETaggerLevel1.isTraining/*&&ParametersForLbjCode.currentParameters.trainingIteration==0*/)
	    		label = ((NEWord)w.previous).neLabel;
	    	else
	    		label = ((NEWord)w.previous).neTypeLevel1;
            for(i = 0; i < w.wordvec.length; i++){
                Double f = w.wordvec[i];
                sense "CCAWordEmbedding" + i + "-" + label : f;
            }
      	}
    }
}

discrete% wordType(NEWord word) <-
{
	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("WordTopicTitleInfo")){
		//System.out.println(word.originalForm+"-"+WordTopicAndLayoutFeatures.getWordType(word));
		sense "" : WordTopicAndLayoutFeatures.getWordType(word); 
	}
}


//discrete% wordType(NEWord word) <-
//{
//	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("WordTopicTitleInfo")){
//		//System.out.println(word.originalForm+"-"+WordTopicAndLayoutFeatures.getWordType(word));
//		sense "" : WordTopicAndLayoutFeatures.getWordType(word);
//	}
//}

//discrete% additionalFeaturesDiscreteNonConjunctive(NEWord word) <-
//{
//
//	for(int fid=0;fid<word.getGeneratedDiscreteFeaturesNonConjunctive().size();fid++){
//		NEWord.DiscreteFeature feature=word.getGeneratedDiscreteFeaturesNonConjunctive().get(fid);
//		if(!feature.useWithinTokenWindow)
//			sense feature.featureGroupName : feature.featureValue;
//	}
//	int i;
//  	NEWord w = word, last = word;
//  	for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
//  	for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;
//
//  	for (; w != last; w = (NEWord) w.next){
//		for(int fid=0;fid<w.getGeneratedDiscreteFeaturesNonConjunctive().size();fid++){
//			NEWord.DiscreteFeature feature=w.getGeneratedDiscreteFeaturesNonConjunctive().get(fid);
//			if(feature.useWithinTokenWindow)
//				sense "pos"+i+"group"+feature.featureGroupName : feature.featureValue;
//		}
//  		i++;
//  	}
//}

//
// these will be used by themselves AND with conjunctions. particularly with the previous predictions
//
//discrete% additionalFeaturesDiscreteConjunctive(NEWord word) <-
//{
//	for(int fid=0;fid<word.getGeneratedDiscreteFeaturesConjunctive().size();fid++){
//		NEWord.DiscreteFeature feature=word.getGeneratedDiscreteFeaturesConjunctive().get(fid);
//		if(!feature.useWithinTokenWindow)
//			sense feature.featureGroupName : feature.featureValue;
//	}
//	int i;
//  	NEWord w = word, last = word;
//  	for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
//  	for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;
//
//  	for (; w != last; w = (NEWord) w.next){
//		for(int fid=0;fid<w.getGeneratedDiscreteFeaturesConjunctive().size();fid++){
//			NEWord.DiscreteFeature feature=w.getGeneratedDiscreteFeaturesConjunctive().get(fid);
//			if(feature.useWithinTokenWindow)
//				sense "pos"+i+"group"+feature.featureGroupName : feature.featureValue;
//		}
//  		i++;
//  	}
//}



//real% additionalFeaturesRealNonConjunctive(NEWord word) <-
//{
//	for(int fid=0;fid<word.getGeneratedRealFeaturesNonConjunctive().size();fid++){
//		NEWord.RealFeature feature=word.getGeneratedRealFeaturesNonConjunctive().get(fid);
//		if(!feature.useWithinTokenWindow)
//			sense feature.featureGroupName : feature.featureValue;
//	}
//	int i;
//  	NEWord w = word, last = word;
//  	for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
//  	for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;
//
//  	for (; w != last; w = (NEWord) w.next){
//		for(int fid=0;fid<w.getGeneratedRealFeaturesNonConjunctive().size();fid++){
//			NEWord.RealFeature feature=w.getGeneratedRealFeaturesNonConjunctive().get(fid);
//			if(feature.useWithinTokenWindow)
//				sense "pos"+i+"group"+feature.featureGroupName : feature.featureValue;
//		}
//  		i++;
//  	}
//}



//
// these will be used by themselves AND with conjunctions. particularly with the previous predictions
//
//real% additionalFeaturesRealConjunctive(NEWord word) <-
//{
//	for(int fid=0;fid<word.getGeneratedRealFeaturesConjunctive().size();fid++){
//		NEWord.RealFeature feature=word.getGeneratedRealFeaturesConjunctive().get(fid);
//		if(!feature.useWithinTokenWindow)
//			sense feature.featureGroupName : feature.featureValue;
//	}
//	int i;
//  	NEWord w = word, last = word;
//  	for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
//  	for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;
//
//  	for (; w != last; w = (NEWord) w.next){
//  		for(int fid=0;fid<w.getGeneratedRealFeaturesConjunctive().size();fid++){
//			NEWord.RealFeature feature=w.getGeneratedRealFeaturesConjunctive().get(fid);
//			if(feature.useWithinTokenWindow)
//				sense "pos"+i+"group"+feature.featureGroupName : feature.featureValue;
//		}
//  		i++;
//  	}
//}


discrete% GazetteersFeatures(NEWord word) <-
{
	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("GazetteersFeatures"))
	{ 
 		int i=0;
   		NEWord w = word, last = (NEWord)word.next;
  
   		for (i = 0; i < 3 && last != null; ++i) last = (NEWord) last.next;
   		for (i = 0; i > -3 && w.previous != null; --i) w = (NEWord) w.previous;
  
 		do 
   		{
	 		if(w.gazetteers!=null)
		 		for(int j=0;j<w.gazetteers.size();j++)
					sense i: w.gazetteers.get(j);
	 		i++;
	 		w = (NEWord) w.next;
   		}while(w != last);
 	}
}


real% WordEmbeddingFeatures(NEWord word) <-
{
  	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("WordEmbeddings"))
	{ 
  		int i;
  		NEWord w = word, last = word;
  		for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
  		for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

  		for (; w != last; w = (NEWord) w.next) {
  			double[] embedding=WordEmbeddings.getEmbedding(w);
			if(embedding!=null)
				for(int dim=0;dim<embedding.length;dim++)
					sense "place"+i+"dim"+dim : embedding[dim];
			i++;
		}
	}
}


discrete% IsSentenceStart(NEWord word) <-
{
    System.out.println("SHOULD NOT BE USING ISSENTENCESTART RIGHT NOW");
	if(word.previous==null)
		sense "start" : "1";
}

/*
discrete%  IsWordCaseNormalized(NEWord word) <-
{
	if(word.isCaseNormalized)
		sense "normalized" : "1";
	//else
	//	sense "normalized" : "0";
}
*/

discrete% Forms(NEWord word) <-
{
  	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("Forms"))
	{ 
  		int i;
  		NEWord w = word, last = word;
  		for (i = 0; i <= 3 && last != null; ++i) last = (NEWord) last.next;
  		for (i = 0; i > -3 && w.previous != null; --i) w = (NEWord) w.previous;

		int startIndex=i;
		NEWord startWord=w;
  		for (; w != last; w = (NEWord) w.next) sense i++ : w.form;
  		i=startIndex;
  		w=startWord;
  		for (; w != last; w = (NEWord) w.next){
			sense i : MyString.normalizeDigitsForFeatureExtraction(w.form);
			i++;
		}
	}
}

discrete% DumbStems(NEWord word) <-
{
  	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("DumbStems"))
	{
         List stems = LanguageSpecificNormalizer.turkish(word.form);
         for(int i = 0; i < stems.size(); i++){
             sense stems.get(i);
         }
	}
}


//real% Linkability(NEWord word) <-
//{
//  	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("Linkability"))
//	{
//  		int i;
//  		NEWord w = word, last = word;
//  		for (i = 0; i <= 1 && last != null; ++i) last = (NEWord) last.next;
//  		for (i = 0; i > -1 && w.previous != null; --i) w = (NEWord) w.previous;
//
//  		for (; w != last; w = (NEWord) w.next){
//			sense "start"+i: w.maxStartLinkabilityScore;
//			sense "end"+i: w.maxEndLinkabilityScore;
//			sense "startPrev"+i: w.maxStartLinkabilityScorePrevalent;
//			sense "endPrev"+i: w.maxEndLinkabilityScorePrevalent;
//			sense "startVeryPrev"+i: w.maxStartLinkabilityScoreVeryPrevalent;
//			sense "endVeryPrev"+i: w.maxEndLinkabilityScoreVeryPrevalent;
//			sense "max"+i: w.maxLinkability;
//			sense "maxPrev"+i: w.maxLinkabilityPrevalent;
//			sense "maxVeryPrev"+i: w.maxLinkabilityVeryPrevalent;
//			sense "start"+i+"_IC": w.maxStartLinkabilityScore_IC;
//			sense "end"+i+"_IC": w.maxEndLinkabilityScore_IC;
//			sense "startPrev"+i+"_IC": w.maxStartLinkabilityScorePrevalent_IC;
//			sense "endPrev"+i+"_IC": w.maxEndLinkabilityScorePrevalent_IC;
//			sense "startVeryPrev"+i+"_IC": w.maxStartLinkabilityScoreVeryPrevalent_IC;
//			sense "endVeryPrev"+i+"_IC": w.maxEndLinkabilityScoreVeryPrevalent_IC;
//			sense "max"+i+"_IC": w.maxLinkability_IC;
//			sense "maxPrev"+i+"_IC": w.maxLinkabilityPrevalent_IC;
//			sense "maxVeryPrev"+i+"_IC": w.maxLinkabilityVeryPrevalent_IC;
//			i++;
//		}
//	}
//}


// Problem 1
discrete% BrownClusterPaths(NEWord word) <-
{
  	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("BrownClusterPaths"))
	{ 
		BrownClusters bc = BrownClusters.get();
  		int i;
  		NEWord w = word, last = word;
  		for (i = 0; i <= 1 && last != null; ++i) last = (NEWord) last.next;
  		for (i = 0; i > -1 && w.previous != null; --i) w = (NEWord) w.previous;

  		for (; w != last; w = (NEWord) w.next){
  			String[] paths=bc.getPrefixes(w);
  			for(int j=0;j<paths.length;j++)
  				sense i : paths[j];
  			i++;
  		}
	}
}


// Problem 1
discrete% FormParts(NEWord word) <-
{
  	if(false && ParametersForLbjCode.currentParameters.featuresToUse.containsKey("Forms")&&
  		ParametersForLbjCode.currentParameters.tokenizationScheme.equals(ParametersForLbjCode.TokenizationScheme.DualTokenizationScheme))
	{ 
		sense "0" : word.form;
		int i=-1;
		int count=-1;
		NEWord w = (NEWord)word.previous;
		while(w!=null&&i>=-2){
			String[] lastParts= w.parts;
			for(int j=0;j<lastParts.length;j++)
			{
				sense count:  MyString.normalizeDigitsForFeatureExtraction(lastParts[j]);
				count--;
			}
			w = (NEWord)w.previous;
			i--;
		}
		i=1;
		count=1;
		w = (NEWord)word.next;
		while(w!=null&&i<=2){
			String[] lastParts= w.parts;
			for(int j=0;j<lastParts.length;j++)
			{
				sense count:  MyString.normalizeDigitsForFeatureExtraction(lastParts[j]);
				count++;
			}
			w = (NEWord)w.next;
			i++;
		}
  	}
}


// Feature set i
discrete{false, true}% Capitalization(NEWord word) <-
{
	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("Capitalization"))
	{ 
	 	int i;
  		NEWord w = word, last = word;
  		for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
  		for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

  		for (; w != last; w = (NEWord) w.next) sense i++ : w.capitalized;
  	}
}

// Feature set ii
discrete{false, true}% WordTypeInformation(NEWord word) <-
{
	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("WordTypeInformation"))
	{ 
	  int i;
	  NEWord w = word, last = word;
	  for (i = 0; i <= 2 && last != null; ++i) last = (NEWord) last.next;
	  for (i = 0; i > -2 && w.previous != null; --i) w = (NEWord) w.previous;

	  for (; w != last; w = (NEWord) w.next, ++i)
	  {
	    boolean allCapitalized = true, allDigits = true, allNonLetters = true;
	
	    for (int j = 0; j < w.form.length(); ++j) {
	    	char c = w.form.charAt(j);
	      	allCapitalized &= Character.isUpperCase(c);
	      	allDigits &= (Character.isDigit(c)||c=='.'||c==',');
	      	allNonLetters &= !Character.isLetter(c);
	    }
	    sense "c" + i : allCapitalized;
	    sense "d" + i : allDigits;
	    sense "p" + i : allNonLetters;
  	  }
  	}
}

// Feature set iii
discrete% Affixes(NEWord word) <-
{

	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("Affixes"))
	{

        int j=0;
        NEWord w = word, last = word;
        for (j = 0; j <= 2 && last != null; ++j) last = (NEWord) last.next;
        for (j = 0; j > -2 && w.previous != null; --j) w = (NEWord) w.previous;

	    for (; w != last; w = (NEWord) w.next){
	      		int N = w.form.length();
        	  	for (int i = 3; i <= 4; ++i)
            		if (N > i) sense "p"+ j +"|" : w.form.substring(0, i);
          		for (int i = 2; i <= 4; ++i)
            		if (N > i) sense "s"+ j +"|" : w.form.substring(N - i);
                j++;
	    }

//        if(ParametersForLbjCode.currentParameters.tokenizationScheme.equals(ParametersForLbjCode.TokenizationScheme.DualTokenizationScheme))
//            for(int i=0;i<word.parts.length;i++)
//                sense "part"+i : word.parts[i];
	}
}


discrete NELabel(NEWord word) <- { return word.neLabel; }

real% nonLocalFeatures(NEWord word) <-
{
	//no need to check which features are active here- if 
	//nonlocal features are not used, they will not be generated! 
	String[] feats=word.getAllNonlocalFeatures();
	for(int i=0;i<feats.length;i++)
		sense feats[i]: word.getNonLocFeatCount(feats[i]);
}


// Feature set iv
// NE tag of previous word.
discrete% PreviousTag1Level1(NEWord word) <-
{
	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("PreviousTag1"))
	{
	  	int i;
	  	NEWord w = word;
	  	if(w.previous!=null)
	  	{
			if (NETaggerLevel1.isTraining/*&&ParametersForLbjCode.currentParameters.trainingIteration==0*/)
	    		sense "-1" : ((NEWord)w.previous).neLabel;
	    	else
	    		sense "-1" : ((NEWord)w.previous).neTypeLevel1;
      	}
    }
}

// Feature set iv
// NE tag of word 2 tokens back
discrete% PreviousTag2Level1(NEWord word) <-
{
	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("PreviousTag2"))
	{ 
	  int i;
	  NEWord w = word;
	  if(w.previous!=null)
	  {
		if(((NEWord)w.previous).previous!=null)
	  	{
			if (NETaggerLevel1.isTraining/*&&ParametersForLbjCode.currentParameters.trainingIteration==0*/) 
		   		sense "-2" : ((NEWord)((NEWord)w.previous).previous).neLabel;
		   	else
		    	sense "-2" : ((NEWord)((NEWord)w.previous).previous).neTypeLevel1;
	 	}
	  }
  	}
}

// Look at 1000 tokens previous, find all occurrences of w,w+1,w+2, calculate ratio of tags.
// not-lexical
real% prevTagsForContextLevel1(NEWord word) <-
{
	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("PrevTagsForContext"))
	{
	  	int i,j;
  		NEWord w = word;
		String[] words=new String[3];
		OccurrenceCounter[] count=new OccurrenceCounter[3];
	  	for (i = 0; i <= 2 && w != null; ++i) {
			count[i]=new OccurrenceCounter();
			words[i]=w.form;
			w = (NEWord) w.next;
		}
			
		w=(NEWord)word.previousIgnoreSentenceBoundary;

		// look back 1000 words
		for(i=0; i<1000 && w != null; i++){
		    // loop over words array
			for(j=0;j<words.length;j++){
				if(words[j] != null && w.form.equals(words[j])){
						if(NETaggerLevel1.isTraining/*&&ParametersForLbjCode.currentParameters.trainingIteration==0*/){
							if(ParametersForLbjCode.currentParameters.prevPredictionsLevel1RandomGenerator.useNoise())
								count[j].addToken(ParametersForLbjCode.currentParameters.prevPredictionsLevel1RandomGenerator.randomLabel());
							else					 
								count[j].addToken(w.neLabel);
						}
						else
			    			count[j].addToken(w.neTypeLevel1);
				}
			}
			w=(NEWord)w.previousIgnoreSentenceBoundary;
		}
	
		for(j=0;j<count.length;j++){
			if(count[j]!=null)	
			{
				String[] all=count[j].getTokens();
				for(i=0;i<all.length;i++)
					sense j+"_"+all[i] : count[j].getCount(all[i])/((double)count[j].totalTokens);	
			}
		}
	}
}

// See nearly identical PreviousTagPatternLevel2 for comments on this.
// (lexical feature)
discrete% PreviousTagPatternLevel1(NEWord word) <-
{
    if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("PreviousTagPatternLevel1")){
        // get label of the previous word
        NEWord w = (NEWord)word.previous;
        String label ="O";
        if(w!=null) {
            if (NETaggerLevel1.isTraining) 
                label = ((NEWord)w).neLabel;
            else
                label = ((NEWord)w).neTypeLevel1;
        } else {
            label = null;
        }

        Vector pattern = new Vector();
        for(int i=0; i<2 && label != null && label.equals("O"); i++ ) {
            pattern.addElement(w.form);
            w = (NEWord)w.previous;
            if(w!=null) {
                if (NETaggerLevel1.isTraining) 
                    label = ((NEWord)w).neLabel;
                else
                    label = ((NEWord)w).neTypeLevel1;
            } else {
                label = null;
            }
        }
        if(pattern.size()>0 && label!=null && !label.equals("O")) {
            label=label.substring(2);
            String res = "";
            for(int i=0;i<pattern.size();i++)
                res=(String) pattern.elementAt(i)+"_"+res;
            res = label+"_"+res;
            sense "" : res;		
        }
	}
}


// Used to have IsSentenceStart here.
mixed% FeaturesSharedTemp(NEWord word) 	<- Capitalization, nonLocalFeatures, GazetteersFeatures, FormParts, Forms,  WordTypeInformation, Affixes, BrownClusterPaths, WordEmbeddingFeatures, posTag, wikifierFeats, PrevEmbedding, NextEmbedding, CurrentEmbedding, DumbStems


mixed% FeaturesLevel1SharedWithLevel2(NEWord word)  <- FeaturesSharedTemp /*,  IsWordCaseNormalized&&FeaturesSharedTemp*/

mixed% FeaturesLevel1Only(NEWord word) <- PreviousTagPatternLevel1, PreviousTag1Level1,PreviousTag2Level1, prevTagsForContextLevel1, PreviousTag1Level1&&Forms  /*PreviousTag1Level1&&BrownClusterPaths, PreviousTag1Level1&&WordEmbeddingFeatures,  PreviousTag1Level1&& HmmEmbeddingFeatures,  PreviousTag1Level1&&additionalFeaturesRealConjunctive, PreviousTag1Level1&& additionalFeaturesDiscreteConjunctive */

discrete NETaggerLevel1(NEWord word)  <-
learn NELabel
  using FeaturesLevel1SharedWithLevel2,FeaturesLevel1Only
  with new SparseNetworkLearner(new WeightedSparseAveragedPerceptron(0.1, 0, 10))
  progressOutput 10000
end

//---------------- CLASSIFIER LEVEL 2 -------------------

// Feature set iv
discrete% PreviousTag1Level2(NEWord word) <-
{
	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("PreviousTag1"))
	{ 
	  	int i;
	  	NEWord w = word;
	  	if(w.previous!=null)
	  	{
			if (NETaggerLevel2.isTraining/*&&ParametersForLbjCode.currentParameters.trainingIteration==0*/) 
	    		sense "-1" : ((NEWord)w.previous).neLabel;
	    	else
	    		sense "-1" : ((NEWord)w.previous).neTypeLevel2;
      	}
    }
}

// Feature set iv
discrete% PreviousTag2Level2(NEWord word) <-
{
	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("PreviousTag2"))
	{ 
	  int i;
	  NEWord w = word;
	  if(w.previous!=null)
	  {
		if(((NEWord)w.previous).previous!=null)
	  	{
			if (NETaggerLevel2.isTraining/*&&ParametersForLbjCode.currentParameters.trainingIteration==0*/) 
		   		sense "-2" : ((NEWord)((NEWord)w.previous).previous).neLabel;
		   	else
		    	sense "-2" : ((NEWord)((NEWord)w.previous).previous).neTypeLevel2;
	 	}
	  }
  	}
}

real% prevTagsForContextLevel2(NEWord word) <-
{
	if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("PrevTagsForContext"))
	{
	  	int i,j;
  		NEWord w = word;
		String[] words=new String[3];
		OccurrenceCounter[] count=new OccurrenceCounter[3];
	  	for (i = 0; i <= 2 && w != null; ++i) {
			count[i]=new OccurrenceCounter();
			words[i]=w.form;
			w = (NEWord) w.next;
		}
			
		w=(NEWord)word.previousIgnoreSentenceBoundary;
		for(i=0;i<1000&&w!=null;i++){
			for(j=0;j<words.length;j++){
				if(words[j]!=null&&w.form.equals(words[j])){
						if(NETaggerLevel2.isTraining/*&&ParametersForLbjCode.currentParameters.trainingIteration==0*/) {
							if(ParametersForLbjCode.currentParameters.prevPredictionsLevel2RandomGenerator.useNoise())
								count[j].addToken(ParametersForLbjCode.currentParameters.prevPredictionsLevel2RandomGenerator.randomLabel());
							else					 
								count[j].addToken(w.neLabel);
						}
						else
			    			count[j].addToken(w.neTypeLevel2);
				}
			}
			w=(NEWord)w.previousIgnoreSentenceBoundary;
		}
	
		for(j=0;j<count.length;j++){
			if(count[j]!=null)	
			{
				String[] all=count[j].getTokens();
				for(i=0;i<all.length;i++)
					sense j+"_"+all[i] : count[j].getCount(all[i])/((double)count[j].totalTokens);	
			}
		}
	}
}

real% Level1AggregationFeatures(NEWord word) <-
{
        if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("PredictionsLevel1")){
        	for(int i=0;i<word.getLevel1AggregationFeatures().size();i++){
        		NEWord.RealFeature f=word.getLevel1AggregationFeatures().get(i);
        		sense f.featureGroupName: f.featureValue;
        	}
        }
}

// This looks at a small window of text (at most 2 tokens) before the word in question.
// If there is a named entity in the window, make feature out of NETag+(w-2)+(w-1)
// This is a lexical feature.
discrete% PreviousTagPatternLevel2(NEWord word) <-
{
    if(ParametersForLbjCode.currentParameters.featuresToUse.containsKey("PreviousTagPatternLevel2")){
        Vector pattern = new Vector();
        String label ="O";

        // think of this block as getting the label of the word.
        // (duplicated below)
        NEWord w = (NEWord)word.previous;
        if(w!=null) {
            if (NETaggerLevel2.isTraining) 
                label = ((NEWord)w).neLabel;
            else
                label = ((NEWord)w).neTypeLevel2;
        } else {
            label = null;
        }

        // look back at most 2 tokens (from word)
        // stop if you reach a labeled token.
        for(int i = 0; i < 2 && label != null && label.equals("O"); i++ ) {
            // w.form is the actual text of the word.
            // never actually add words NOT labeled O (ie, named entities)
            pattern.addElement(w.form);
            w = (NEWord)w.previous;
            if(w!=null) {
                if (NETaggerLevel2.isTraining) 
                    label = ((NEWord)w).neLabel;
                else
                    label = ((NEWord)w).neTypeLevel2;
            } else {
                label = null;
            }
        }
        
        if(pattern.size()>0&&label!=null&&!label.equals("O")) {
            // presumably this captures B-, or I-, or O-, etc.
            label=label.substring(2); 
            String res = "";
            for(int i=0;i<pattern.size();i++)
                res=(String) pattern.elementAt(i)+"_"+res;
            res = label+"_"+res;
            sense "" : res;

        }
	}
}



mixed% FeaturesLevel2(NEWord word) <-   PreviousTagPatternLevel2, Level1AggregationFeatures,  PreviousTag1Level2, PreviousTag2Level2 ,  prevTagsForContextLevel2, PreviousTag1Level2&&Forms /* PreviousTag1Level2&&BrownClusterPaths, PreviousTag1Level2&&WordEmbeddingFeatures, PreviousTag1Level2&&HmmEmbeddingFeatures,  PreviousTag1Level2&&additionalFeaturesDiscreteConjunctive, PreviousTag1Level2&&additionalFeaturesRealConjunctive*/
																	

discrete NETaggerLevel2(NEWord word)  <-
learn NELabel 
  using FeaturesLevel1SharedWithLevel2,FeaturesLevel2
  with new SparseNetworkLearner(new WeightedSparseAveragedPerceptron(.1, 0, 20))
end

